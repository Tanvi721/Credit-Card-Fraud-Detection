{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf679f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:27:28 INFO mlflow.tracking.fluent: Experiment with name 'iris_classification' does not exist. Creating a new experiment.\n",
      "2025/08/06 22:27:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/06 22:28:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Experiment Name: iris_classification\n",
      "MLflow Tracking URI: http://localhost:5000\n",
      "MLflow Run ID: ccac4b28ea8542e29316a291ab8665a0\n",
      "Experiment ID: 444229957778145889\n",
      "MLflow UI URL: http://localhost:5000\n",
      "Run URL: http://localhost:5000/#/experiments/444229957778145889/runs/ccac4b28ea8542e29316a291ab8665a0\n",
      "üèÉ View run sincere-shrike-94 at: http://localhost:5000/#/experiments/444229957778145889/runs/ccac4b28ea8542e29316a291ab8665a0\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/444229957778145889\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "=== Feature Importance ===\n",
      "             feature  importance\n",
      "2  petal length (cm)    0.452175\n",
      "3   petal width (cm)    0.431695\n",
      "0  sepal length (cm)    0.106226\n",
      "1   sepal width (cm)    0.009903\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Set MLflow tracking URI (optional - defaults to local)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Set MLflow experiment\n",
    "experiment_name = \"iris_classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Parameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 3\n",
    "    random_state = 42\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"train_size\", len(X_train))\n",
    "    mlflow.log_metric(\"test_size\", len(X_test))\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(rf, \"random_forest_model\")\n",
    "    \n",
    "    # Log dataset info\n",
    "    mlflow.log_param(\"features\", iris.feature_names)\n",
    "    mlflow.log_param(\"target_names\", iris.target_names.tolist())\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Experiment Name: {experiment_name}\")\n",
    "    print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")\n",
    "    print(f\"Experiment ID: {mlflow.active_run().info.experiment_id}\")\n",
    "    print(f\"MLflow UI URL: {mlflow.get_tracking_uri()}\")\n",
    "    print(f\"Run URL: {mlflow.get_tracking_uri()}/#/experiments/{mlflow.active_run().info.experiment_id}/runs/{mlflow.active_run().info.run_id}\")\n",
    "\n",
    "# View results\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "print(\"\\n=== Feature Importance ===\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': iris.feature_names,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:04:45 INFO mlflow.tracking.fluent: Experiment with name 'credit_card_fraud_detection' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment 'credit_card_fraud_detection' is ready!\n",
      "üöÄ Starting Credit Card Fraud Detection ML Pipeline\n",
      "============================================================\n",
      "Generating synthetic credit card dataset...\n",
      "Generated synthetic dataset with shape: (10000, 8)\n",
      "Fraud cases: 200.0 (2.00%)\n",
      "Preprocessing data...\n",
      "Training set shape: (8000, 7)\n",
      "Test set shape: (2000, 7)\n",
      "Initialized 5 models for training\n",
      "Training and evaluating models...\n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:04:48 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/06 22:05:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9870, F1: 0.5938, AUC: 0.9845\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:05:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/06 22:05:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Accuracy: 0.9850, F1: 0.5000, AUC: 0.9580\n",
      "\n",
      "Training Gradient Boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:06:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/06 22:06:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - Accuracy: 0.9825, F1: 0.4776, AUC: 0.9721\n",
      "\n",
      "Training SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/06 22:06:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/06 22:07:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.9865, F1: 0.5091, AUC: 0.9399\n",
      "\n",
      "Training Neural Network...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CreditCardMLPipeline:\n",
    "    def __init__(self, experiment_name=\"credit_card_fraud_detection\"):\n",
    "        \"\"\"Initialize the ML pipeline with MLflow experiment\"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Set up MLflow\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"MLflow experiment '{experiment_name}' is ready!\")\n",
    "    \n",
    "    def load_dataset(self, dataset_path=None):\n",
    "        \"\"\"Load credit card dataset automatically\"\"\"\n",
    "        if dataset_path:\n",
    "            # Load custom dataset\n",
    "            self.data = pd.read_csv('C:/Users/Pramod/OneDrive/Desktop/Project/creditcard.csv')\n",
    "            print(f\"Loaded custom dataset with shape: {self.data.shape}\")\n",
    "        else:\n",
    "            # Generate synthetic credit card fraud dataset\n",
    "            print(\"Generating synthetic credit card dataset...\")\n",
    "            np.random.seed(42)\n",
    "            n_samples = 10000\n",
    "            \n",
    "            # Generate features\n",
    "            data = {\n",
    "                'V1': np.random.normal(0, 1, n_samples),\n",
    "                'V2': np.random.normal(0, 1, n_samples),\n",
    "                'V3': np.random.normal(0, 1, n_samples),\n",
    "                'V4': np.random.normal(0, 1, n_samples),\n",
    "                'V5': np.random.normal(0, 1, n_samples),\n",
    "                'Amount': np.random.exponential(50, n_samples),\n",
    "                'Time': np.random.uniform(0, 172800, n_samples)  # 48 hours in seconds\n",
    "            }\n",
    "            \n",
    "            # Create fraud labels (imbalanced - 2% fraud)\n",
    "            fraud_indices = np.random.choice(n_samples, int(0.02 * n_samples), replace=False)\n",
    "            data['Class'] = np.zeros(n_samples)\n",
    "            data['Class'][fraud_indices] = 1\n",
    "            \n",
    "            # Make fraudulent transactions different\n",
    "            for idx in fraud_indices:\n",
    "                data['V1'][idx] += np.random.normal(2, 0.5)\n",
    "                data['V2'][idx] += np.random.normal(-1.5, 0.5)\n",
    "                data['Amount'][idx] *= np.random.uniform(0.1, 0.3)\n",
    "            \n",
    "            self.data = pd.DataFrame(data)\n",
    "            print(f\"Generated synthetic dataset with shape: {self.data.shape}\")\n",
    "            print(f\"Fraud cases: {self.data['Class'].sum()} ({self.data['Class'].mean()*100:.2f}%)\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the dataset\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = self.data.drop('Class', axis=1)\n",
    "        y = self.data['Class']\n",
    "        \n",
    "        # Handle any categorical variables if present\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        print(f\"Training set shape: {self.X_train.shape}\")\n",
    "        print(f\"Test set shape: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train_scaled, self.X_test_scaled, self.y_train, self.y_test\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize 5 different ML models\"\"\"\n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'SVM': SVC(probability=True, random_state=42),\n",
    "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000)\n",
    "        }\n",
    "        print(f\"Initialized {len(self.models)} models for training\")\n",
    "        return self.models\n",
    "    \n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"Train all models and log results to MLflow\"\"\"\n",
    "        print(\"Training and evaluating models...\")\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            with mlflow.start_run(run_name=model_name):\n",
    "                # Train model\n",
    "                model.fit(self.X_train_scaled, self.y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(self.X_test_scaled)\n",
    "                y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                precision = precision_score(self.y_test, y_pred, zero_division=0)\n",
    "                recall = recall_score(self.y_test, y_pred, zero_division=0)\n",
    "                f1 = f1_score(self.y_test, y_pred, zero_division=0)\n",
    "                \n",
    "                # AUC score if probability predictions available\n",
    "                auc = roc_auc_score(self.y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "                \n",
    "                # Cross-validation score\n",
    "                cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5, scoring='f1')\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                # Store results\n",
    "                self.results[model_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'auc': auc,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'model': model\n",
    "                }\n",
    "                \n",
    "                # Log parameters to MLflow\n",
    "                if hasattr(model, 'get_params'):\n",
    "                    params = model.get_params()\n",
    "                    for param, value in params.items():\n",
    "                        mlflow.log_param(param, value)\n",
    "                \n",
    "                # Log metrics to MLflow\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"precision\", precision)\n",
    "                mlflow.log_metric(\"recall\", recall)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "                if auc is not None:\n",
    "                    mlflow.log_metric(\"auc\", auc)\n",
    "                mlflow.log_metric(\"cv_mean_f1\", cv_mean)\n",
    "                mlflow.log_metric(\"cv_std_f1\", cv_std)\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "                \n",
    "                auc_str = f\"{auc:.4f}\" if auc is not None else \"N/A\"\n",
    "                print(f\"{model_name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC: {auc_str}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def display_results_summary(self):\n",
    "        \"\"\"Display comprehensive results summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame.from_dict(self.results, orient='index')\n",
    "        results_df = results_df.round(4)\n",
    "        \n",
    "        # Sort by F1 score (important for imbalanced dataset)\n",
    "        results_df_sorted = results_df.sort_values('f1', ascending=False)\n",
    "        \n",
    "        print(results_df_sorted[['accuracy', 'precision', 'recall', 'f1', 'auc', 'cv_mean']].to_string())\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = results_df_sorted.index[0]\n",
    "        best_model = self.results[best_model_name]['model']\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "        print(f\"   F1 Score: {results_df_sorted.loc[best_model_name, 'f1']:.4f}\")\n",
    "        print(f\"   Accuracy: {results_df_sorted.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "        print(f\"   AUC: {results_df_sorted.loc[best_model_name, 'auc']:.4f}\")\n",
    "        \n",
    "        # Classification report for best model\n",
    "        y_pred_best = best_model.predict(self.X_test_scaled)\n",
    "        print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "        print(classification_report(self.y_test, y_pred_best))\n",
    "        \n",
    "        return results_df_sorted, best_model_name\n",
    "    \n",
    "    def plot_model_comparison(self):\n",
    "        \"\"\"Create visualization comparing model performance\"\"\"\n",
    "        # Prepare data for plotting\n",
    "        model_names = list(self.results.keys())\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            ax = axes[i//2, i%2]\n",
    "            values = [self.results[model][metric] for model in model_names]\n",
    "            \n",
    "            bars = ax.bar(model_names, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "            ax.set_title(f'{metric.capitalize()} Comparison', fontweight='bold')\n",
    "            ax.set_ylabel(metric.capitalize())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Log plot to MLflow\n",
    "        with mlflow.start_run(run_name=\"model_comparison\"):\n",
    "            plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "            mlflow.log_artifact(\"model_comparison.png\")\n",
    "    \n",
    "    def run_complete_pipeline(self, dataset_path=None):\n",
    "        \"\"\"Run the complete ML pipeline\"\"\"\n",
    "        print(\"üöÄ Starting Credit Card Fraud Detection ML Pipeline\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load dataset\n",
    "        self.load_dataset(dataset_path)\n",
    "        \n",
    "        # Step 2: Preprocess data\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Step 3: Initialize models\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Step 4: Train and evaluate models\n",
    "        self.train_and_evaluate_models()\n",
    "        \n",
    "        # Step 5: Display results\n",
    "        results_df, best_model = self.display_results_summary()\n",
    "        \n",
    "        # Step 6: Create visualizations\n",
    "        self.plot_model_comparison()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "        print(f\"üî¨ Check MLflow UI with: mlflow ui\")\n",
    "        print(f\"üìä Experiment: {self.experiment_name}\")\n",
    "        \n",
    "        return results_df, best_model\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run the pipeline\n",
    "    pipeline = CreditCardMLPipeline()\n",
    "    \n",
    "    # Option 1: Run with synthetic data\n",
    "    results, best_model_name = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    # Option 2: Run with your own dataset (uncomment below)\n",
    "    # results, best_model_name = pipeline.run_complete_pipeline(\"path/to/your/creditcard.csv\")\n",
    "    \n",
    "    print(f\"\\nüéØ Best performing model: {best_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
